{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Digit-Recognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Gregory-Eales/NeuralNet-Digit-Recognition/blob/master/Digit_Recognition.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "ii2wESXNnl-S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import dependencies\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iaZ1bHtRrkIe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load digit data set\n",
        "x, uncleaned_y = datasets.load_digits(return_X_y=True)\n",
        "\n",
        "x = x/10\n",
        "# create a y for each classification: numbers 0-9 and stores it in 'answers'\n",
        "answers = []\n",
        "for i in range(10):\n",
        "  zero = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "  zero[i] = 1\n",
        "  answers.append(zero)\n",
        "\n",
        "# iterate through 'uncleaned_y' and add the correct classification for each y\n",
        "y = []\n",
        "for i in uncleaned_y:\n",
        "  for j in range(10):\n",
        "    if i == j:\n",
        "      y.append(answers[j])\n",
        "\n",
        "# convert y to an array      \n",
        "y = np.array(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BBZvWKSyQDuV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "2694a425-a43e-4a60-f2eb-4d2437ffcc48"
      },
      "cell_type": "code",
      "source": [
        "print()"
      ],
      "execution_count": 650,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.  0.  0.4 ... 0.1 0.  0. ]\n",
            " [0.  0.  0.4 ... 1.5 0.5 0. ]\n",
            " [0.  0.  0.7 ... 0.  0.  0. ]\n",
            " ...\n",
            " [0.  0.  0.1 ... 0.6 0.  0. ]\n",
            " [0.  0.  0.2 ... 1.2 0.  0. ]\n",
            " [0.  0.  1.  ... 1.2 0.1 0. ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HVLUZ-IxoKjL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define neural network model\n",
        "class NeuralNetwork(object):\n",
        "  \n",
        "  def __init__(self, x, y, alpha=0.1, iterations=1000, num_layers=3, hidden_addition=1):\n",
        "    \n",
        "    # initiate class properties\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    self.alpha = alpha\n",
        "    self.iterations = iterations\n",
        "    self.num_layers = num_layers\n",
        "    self.w = {}\n",
        "    self.b = {}\n",
        "    self.z = {}\n",
        "    self.a = {}\n",
        "    self.e = {}\n",
        "    self.historical_cost = []\n",
        "    \n",
        "    # create layer weights and layer variables\n",
        "    for i in range(1, self.num_layers+hidden_addition):\n",
        "      \n",
        "      # if not the last weight initiate with eacher layer one bigger than the input size of x\n",
        "      \n",
        "      \n",
        "      if i != self.num_layers:\n",
        "        if i == 1:\n",
        "          self.w[\"w\"+str(i)] = np.random.randn(self.x.shape[1], self.x.shape[1]+hidden_addition)*0.5\n",
        "          self.b[\"b\"+str(i)] = np.random.randn(1, self.x.shape[1]+hidden_addition)*0.5\n",
        "        else:  \n",
        "          self.w[\"w\"+str(i)] = np.random.randn(self.x.shape[1]+hidden_addition, self.x.shape[1]+hidden_addition)*0.5\n",
        "          self.b[\"b\"+str(i)] = np.random.randn(1, self.x.shape[1]+hidden_addition)*0.5\n",
        "      \n",
        "      # if the last weight initiate with output = dimensions of y\n",
        "      else:\n",
        "        self.w[\"w\"+str(i)] = np.random.randn(self.x.shape[1]+hidden_addition, self.y.shape[1])*0.1\n",
        "        self.b[\"b\"+str(i)] = np.random.randn(1, self.y.shape[1])*0.1\n",
        "      \n",
        "      # doesnt matter, will be changed later                                               \n",
        "      self.z[\"z\" + str(i)] = np.zeros([self.x.shape[0], self.x.shape[1]])\n",
        "      self.a[\"a\" + str(i)] = np.zeros([self.x.shape[0], self.x.shape[1]])\n",
        "    \n",
        "  # calculate and make predictions\n",
        "  def forward_propagation(self):\n",
        "    \n",
        "    # initiate forward propigation with dotting x an w1\n",
        "    self.a['a0'] = self.x\n",
        "    self.z[\"z1\"] = np.dot(self.x, self.w[\"w1\"]) + self.b['b1']\n",
        "    self.a['a1'] = np.tanh(self.z['z1'])\n",
        "    \n",
        "    # iterate through all dots and activations\n",
        "    for i in range(2, self.num_layers):\n",
        "      self.z[\"z\" + str(i)] = np.dot(self.a['a'+str(i-1)], self.w['w' + str(i)]) + self.b['b' + str(i)]\n",
        "      self.a['a'+ str(i)] = np.tanh(self.z[\"z\" + str(i)])\n",
        "    \n",
        "    # on the last iteration use sigmoid instead of tanh for classification\n",
        "    self.z[\"z\" + str(self.num_layers)] = np.dot(self.a['a'+str(self.num_layers-1)], self.w['w' + str(self.num_layers)]) + self.b['b' + str(self.num_layers)]\n",
        "    self.a['a'+ str(self.num_layers)] = self.sigmoid(self.z[\"z\" + str(self.num_layers)])\n",
        "    return self.a['a'+str(self.num_layers)]\n",
        "    \n",
        "  # adjust weights based on cost function\n",
        "  def backward_propagation(self):\n",
        "    self.create_updates()\n",
        "    #self.w['w3'] = self.w['w3'] - (self.alpha/self.x.shape[0])*np.dot((self.sigmoid_prime(self.z['z3'])*self.j_prime()).T, self.a['a2']).T\n",
        "    \n",
        "    # iterate throught weights\n",
        "    for i in reversed(range(1, self.num_layers+1)):\n",
        "      self.w[\"w\"+str(i)] = self.w[\"w\"+str(i)] - (self.alpha/self.x.shape[0])*np.dot(self.a['a'+str(i-1)].T, self.e['e'+str(i)]) \n",
        "      self.b[\"b\"+str(i)] = self.b[\"b\"+str(i)] - (self.alpha/self.x.shape[0])*np.sum(self.e['e'+str(i)], axis=0)\n",
        "                                                                                                                                 \n",
        "  \n",
        "  # returns the derivative of the cost\n",
        "  def j_prime(self):\n",
        "    return (self.y/self.a['a' + str(self.num_layers)] - (1-self.y)/(1-self.a['a' + str(self.num_layers)]))\n",
        "  \n",
        "  # creates update cache\n",
        "  def create_updates(self):\n",
        "    self.e['e'+str(self.num_layers)] = self.j_prime()*self.sigmoid_prime(self.z['z'+str(self.num_layers)])\n",
        "    for i in reversed(range(1, self.num_layers)):\n",
        "      self.e['e'+str(i)] =  np.dot(self.e['e' + str(i+1)], self.w['w' + str(i+1)].T)*self.tanh_prime(self.z['z'+str(i)])\n",
        "      \n",
        "      \n",
        "  # optimize model based on inputes\n",
        "  def optimize(self):\n",
        "    \n",
        "    for i in range(self.iterations):\n",
        "      if i%100 == 0:\n",
        "        print(str(i/10) + \"%\")\n",
        "      self.forward_propagation()\n",
        "      self.backward_propagation()\n",
        "      self.historical_cost.append(self.cost_function())\n",
        "    print(\"Complete\")\n",
        "    \n",
        "  # calculate the cost of prections\n",
        "  def cost_function(self):\n",
        "    j = -(np.sum(self.y*np.log(self.a['a'+str(self.num_layers)]) + (1-self.y)*np.log(1 - self.a['a'+ str(self.num_layers)]))/self.x.shape[0])\n",
        "    return j\n",
        "  \n",
        "  # sigmoid activation function\n",
        "  def sigmoid(self, z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "    \n",
        "  # derivative of sigmoid activation function\n",
        "  def sigmoid_prime(self, z):\n",
        "    return -np.exp(-z)/np.square(1 + np.exp(-z))\n",
        "  \n",
        "  # derivative of tanh activation function\n",
        "  def tanh_prime(self, z):\n",
        "    return 1 - np.square(np.tanh(z))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jHXVy8LzoKtD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NN = NeuralNetwork(x[0:1700], y[0:1700], alpha=0.12, iterations=1000, num_layers=5, hidden_addition=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cPBk0gwO_qC8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d13835ca-3535-49c4-9be9-02203f052e3b"
      },
      "cell_type": "code",
      "source": [
        "NN.optimize()"
      ],
      "execution_count": 646,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0%\n",
            "10.0%\n",
            "20.0%\n",
            "30.0%\n",
            "40.0%\n",
            "50.0%\n",
            "60.0%\n",
            "70.0%\n",
            "80.0%\n",
            "90.0%\n",
            "Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ci8LLlJxL0Pi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_axis = []\n",
        "for i in range(NN.iterations):\n",
        "  x_axis.append(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BuVnjCcEoK0j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "outputId": "32cc5b8d-8b5b-45db-cfac-c6755638896a"
      },
      "cell_type": "code",
      "source": [
        "plt.plot(x_axis, NN.historical_cost)\n",
        "plt.show()\n",
        "print(NN.historical_cost)\n",
        "NN.x = x[1700:x.shape[0]]\n",
        "NN.y = y[1700:x.shape[0]]\n",
        "predictions = (NN.forward_propagation())\n",
        "p = np.zeros_like(predictions)\n",
        "p[np.arange(len(predictions)), predictions.argmax(1)] = 1\n",
        "\n",
        "print(p[30:40])\n",
        "print(y[30:40])\n"
      ],
      "execution_count": 651,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdkAAAFKCAYAAABRtSXvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIWQBAEgTQhYAgDT5/0S4JJ0hrDFVAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f78d5d09c50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[7.455627013244632, 4.740018674680526, 3.765332396648948, 3.358942736194338, 3.1667529844572737, 3.0542618345548944, 2.9721587469709827, 2.903266941204761, 2.840877141032139, 2.782324895687117, 2.7264784124523933, 2.672780226985641, 2.6209329518407074, 2.5707607680102544, 2.5221421225215988, 2.4750032650850695, 2.429291172077051, 2.3849238051957755, 2.3417991082406986, 2.299826326625253, 2.258954941969797, 2.219187361919633, 2.1805527114592893, 2.1430723632337583, 2.106761694498991, 2.0716525206307197, 2.0377704138360744, 2.005095477007677, 1.973578743120659, 1.94316129091793, 1.9137773992570697, 1.8853585584562078, 1.8578423231959922, 1.8311782059582824, 1.805325312815098, 1.7802459119130662, 1.755902058687688, 1.7322556991969922, 1.7092695152546389, 1.6869072696088503, 1.6651338700581757, 1.643915615089, 1.6232203165666323, 1.6030168773588138, 1.5832749159866222, 1.563965586380756, 1.5450641383643942, 1.5265522645409475, 1.508415323959393, 1.4906361734476286, 1.4731944191004451, 1.4560696515353182, 1.4392435608839078, 1.422700777499639, 1.4064291211700393, 1.390419385502999, 1.3746647628572057, 1.3591601086222647, 1.3439012308995322, 1.3288843370420163, 1.3141057184405147, 1.2995616650807134, 1.2852484324078421, 1.271162032786206, 1.2572979559134576, 1.2436512759787317, 1.2302173588354925, 1.2169927417141135, 1.2039753888010025, 1.1911638917645364, 1.1785561951457366, 1.166148888248426, 1.153937346765042, 1.14191625333613, 1.1300800725878186, 1.1184233754111834, 1.1069410720704267, 1.0956286230562198, 1.084482225276649, 1.073498877570478, 1.0626761895776589, 1.0520118944059473, 1.041503238264979, 1.0311465576395464, 1.0209372488732977, 1.0108700786333769, 1.000939613752611, 0.9911405752745717, 0.9814680635000939, 0.9719177144930913, 0.9624858806976901, 0.953169918003752, 0.9439685911091844, 0.9348823989896207, 0.9259133783147597, 0.9170640983180073, 0.9083362708404711, 0.8997298360015511, 0.8912428967106903, 0.8828721948594386, 0.8746136965124401, 0.8664630878665281, 0.8584162063379116, 0.8504695311295926, 0.8426208007208537, 0.8348695445941611, 0.8272169757954, 0.819664944169961, 0.8122146295150852, 0.80486595239551, 0.7976177720791974, 0.790468323000446, 0.7834155417857469, 0.776457245214938, 0.7695912187878311, 0.7628152646368809, 0.756127231762507, 0.7495250368488308, 0.7430066790093198, 0.7365702512195955, 0.7302139512445338, 0.7239360942719356, 0.7177351282394379, 0.711609651175875, 0.7055584279964247, 0.6995804028449328, 0.6936747032312324, 0.687840633593466, 0.6820776566548089, 0.6763853601699555, 0.6707634070287957, 0.6652114715427963, 0.6597291722202723, 0.6543160151051333, 0.6489713587852861, 0.6436944056407674, 0.6384842177750791, 0.633339751530789, 0.6282599017614655, 0.623243546751962, 0.6182895867764326, 0.6133969724953983, 0.6085647222689115, 0.6037919292751102, 0.5990777601092805, 0.5944214466882479, 0.589822273184694, 0.5852795595944076, 0.5807926434632485, 0.5763608612364196, 0.5719835305790786, 0.5676599348164831, 0.5633893103578422, 0.5591708376365421, 0.555003635764853, 0.5508867607996437, 0.5468192072702281, 0.5427999124438465, 0.5388277627053022, 0.5349016014045411, 0.531020237571125, 0.5271824549937738, 0.523387021302633, 0.5196326968628933, 0.515918243487974, 0.5122424332076342, 0.5086040575740737, 0.5050019382362311, 0.5014349397170537, 0.4979019854227496, 0.4944020777977115, 0.49093432307596097, 0.4874979600938351, 0.4840923909509905, 0.480717208931615, 0.4773722164870589, 0.474057424490032, 0.47077302532683246, 0.4675193381504878, 0.46429673381109704, 0.4611055553619516, 0.4579460523671354, 0.45481834163323726, 0.4517223968975768, 0.44865806125575813, 0.44562507231930626, 0.44262309100153896, 0.43965172801637703, 0.4367105653929286, 0.4337991724990113, 0.4309171171570513, 0.42806397278152, 0.42523932245588786, 0.422442760761945, 0.4196738940962202, 0.41693234016956043, 0.4142177273363446, 0.41152969427432884, 0.4088678902985955, 0.40623197626350394, 0.40362162566248977, 0.40103652528733164, 0.39847637475728936, 0.39594088442194303, 0.39342977154281294, 0.39094275514613697, 0.3884795503454585, 0.3860398631152356, 0.38362338640195887, 0.3812297981431665, 0.3788587613557282, 0.37650992608623685, 0.3741829327719074, 0.3718774164580081, 0.36959301132792777, 0.3673293550774388, 0.36508609276403065, 0.36286287985948723, 0.3606593843187448, 0.3584752875511295, 0.35631028424829597, 0.35416408109646375, 0.352036394489829, 0.3499269474782799, 0.34783546633599277, 0.3457616773361239, 0.34370530456191595, 0.34166606985953035, 0.3396436962841131, 0.3376379164659973, 0.33564848695579724, 0.33367520839394754, 0.33171794894428935, 0.32977666505804804, 0.32785141083659164, 0.32594232798903755, 0.3240496150061349, 0.32217348475871244, 0.32031412699740097, 0.31847168943221443, 0.3166462797037063, 0.3148379794013237, 0.31304685745800487, 0.31127297388003927, 0.3095163716256146, 0.3077770604295471, 0.3060549993113836, 0.3043500840540186, 0.3026621431596518, 0.30099094245730257, 0.2993361961641635, 0.29769758135554714, 0.29607475314720555, 0.2944673587584417, 0.29287504945297593, 0.29129748989853915, 0.28973436474489206, 0.2881853823178772, 0.28665027541245847, 0.2851287993452807, 0.2836207277174148, 0.28212584666708834, 0.2806439486276483, 0.279174826628932, 0.27771826995788207, 0.276274061602223, 0.27484197747951933, 0.2734217871319727, 0.27201325540796967, 0.270616144645204, 0.2692302169647967, 0.26785523642216963, 0.2664909708951537, 0.265137193700546, 0.2637936850105559, 0.26246023319201967, 0.26113663621784466, 0.2598227033044965, 0.25851825691109953, 0.25722313519108403, 0.255937194909678, 0.2546603147230681, 0.2533923985561677, 0.2521333786280638, 0.250883217493725, 0.24964190836234276, 0.24840947299960298, 0.24718595679075972, 0.24597142103607308, 0.2447659331694065, 0.24356955614324738, 0.24238233850505728, 0.24120430659067066, 0.2400354598271713, 0.23887576954428413, 0.237725181141956, 0.23658361908562373, 0.23545099402513742, 0.23432721129943954, 0.2332121801050538, 0.23210582259439705, 0.23100808209505544, 0.22991892952503912, 0.22883836700033244, 0.22776642770876526, 0.22670317147312172, 0.22564867608588937, 0.22460302536338528, 0.22356629567929026, 0.22253854317635524, 0.22151979370642338, 0.22051003684298487, 0.21950922431796283, 0.2185172723172534, 0.21753406649496548, 0.2165594684090708, 0.2155933222500709, 0.21463546106754022, 0.2136857120523454, 0.21274390071860336, 0.2118098540200509, 0.21088340253833662, 0.2099643819191696, 0.20905263373091498, 0.20814800589865715, 0.20725035283791238, 0.20635953538352275, 0.20547542058434987, 0.2045978814144036, 0.20372679643589817, 0.20286204943875716, 0.2020035290734431, 0.20115112848884498, 0.20030474498360182, 0.19946427967710667, 0.19862963720508023, 0.1978007254437164, 0.1969774552657482, 0.19615974033122904, 0.1953474969152717, 0.19454064377438857, 0.19373910205241907, 0.19294279522629545, 0.19215164909111007, 0.19136559178310147, 0.19058455383829473, 0.18980846828361597, 0.18903727075636684, 0.1882708996469968, 0.18750929625916463, 0.18675240498014883, 0.1860001734537788, 0.18525255274724187, 0.18450949750242837, 0.18377096606195906, 0.1830369205597654, 0.1823073269661411, 0.1815821550776283, 0.180861378443015, 0.18014497421816933, 0.17943292294443786, 0.17872520824790172, 0.1780218164598448, 0.17732273616223357, 0.17662795766566913, 0.17593747243090885, 0.17525127244840785, 0.174569349593115, 0.17389169497370513, 0.17321829829633564, 0.17254914726274007, 0.17188422702100561, 0.17122351968480595, 0.1705670039333842, 0.16991465470047956, 0.16926644295600696, 0.16862233557996395, 0.16798229532407513, 0.16734628085333886, 0.16671424685709074, 0.1660861442175382, 0.1654619202229625, 0.16484151881289555, 0.16422488084348036, 0.16361194436283027, 0.16300264488843502, 0.16239691568146825, 0.16179468801619612, 0.16119589144657645, 0.16060045407657716, 0.16000830284573506, 0.1594193638469715, 0.1588335626995335, 0.15825082500580082, 0.15767107692596583, 0.1570942459081836, 0.15652026161201688, 0.15594905705739537, 0.15538057001657588, 0.15481474463888328, 0.15425153325385113, 0.15369089823660595, 0.15313281374432453, 0.1525772670581481, 0.15202425921699927, 0.15147380464390187, 0.15092592957501486, 0.15038066931579092, 0.14983806462908042, 0.14929815781522013, 0.1487609891598659, 0.1482265943254104, 0.1476950029635411, 0.1471662384445839, 0.1466403182876459, 0.14611725474600432, 0.14559705507562548, 0.1450797212289945, 0.14456524897170603, 0.1440536266258697, 0.14354483375144358, 0.143038840077476, 0.14253560491683132, 0.14203507718279354, 0.14153719601455753, 0.14104189193808003, 0.1405490884489989, 0.14005870390119887, 0.13957065360608725, 0.13908485207986665, 0.13860121540717157, 0.1381196637109824, 0.13764012372567225, 0.1371625314598887, 0.1366868349081904, 0.13621299672643125, 0.13574099672999354, 0.1352708340141043, 0.13480252844424473, 0.134336121238923, 0.13387167438522043, 0.13340926870399072, 0.13294900051955275, 0.13249097707344304, 0.13203531101911986, 0.13158211449745064, 0.1311314933762026, 0.1306835422143929, 0.13023834038832904, 0.1297959496239357, 0.1293564129692824, 0.1289197550598122, 0.12848598340759323, 0.12805509039377636, 0.12762705565067722, 0.1272018485669218, 0.12677943071491593, 0.1263597580677124, 0.12594278293159214, 0.12552845556657655, 0.12511672549903963, 0.1247075425503438, 0.12430085761571101, 0.1238966232311382, 0.12349479396546749, 0.1230953266715617, 0.12269818062621152, 0.12230331758376183, 0.12191070176399248, 0.12152029979079637, 0.12113208059477128, 0.12074601528999797, 0.12036207703298082, 0.11998024086990473, 0.11960048357695337, 0.11922278349735325, 0.11884712037800069, 0.11847347520792631, 0.11810183006041346, 0.11773216794026242, 0.11736447263745345, 0.11699872858827717, 0.11663492074484803, 0.11627303445378052, 0.1159130553446754, 0.11555496922892546, 0.11519876200920551, 0.11484441959985806, 0.114491927858227, 0.11414127252682983, 0.11379243918610482, 0.11344541321732167, 0.11310017977511778, 0.11275672376901431, 0.1124150298531866, 0.11207508242371089, 0.11173686562248673, 0.11140036334703882, 0.1110655592654339, 0.11073243683560109, 0.11040097932841675, 0.1100711698540019, 0.1097429913907754, 0.10941642681690995, 0.10909145894393994, 0.10876807055237271, 0.10844624442925198, 0.10812596340771015, 0.10780721040862572, 0.10748996848456766, 0.1071742208662617, 0.10685995101184555, 0.10654714265919797, 0.10623577988161759, 0.1059258471470958, 0.10561732938136677, 0.10531021203482391, 0.10500448115326202, 0.10470012345223595, 0.10439712639461446, 0.10409547827065338, 0.10379516827961521, 0.1034961866116255, 0.10319852452809043, 0.10290217443861435, 0.10260712997197362, 0.10231338603834411, 0.10202093887968103, 0.10172978610494333, 0.10143992670678485, 0.10115136105643907, 0.10086409087384056, 0.1005781191705824, 0.10029345016411265, 0.1000100891626163, 0.09972804242127759, 0.09944731697199845, 0.09916792043008434, 0.09888986078277853, 0.09861314616571563, 0.09833778463425917, 0.09806378393718845, 0.09779115130024682, 0.09751989322663232, 0.0972500153206374, 0.09698152213939452, 0.09671441707617377, 0.09644870227703703, 0.0961843785910178, 0.09592144555249324, 0.09565990139314445, 0.0953997430799294, 0.09514096637485099, 0.09488356591198631, 0.0946275352872205, 0.0943728671563499, 0.09411955333762181, 0.09386758491530192, 0.09361695234144486, 0.093367645533641, 0.0931196539670849, 0.09287296675982969, 0.0926275727505447, 0.09238346056846816, 0.09214061869554824, 0.09189903552099453, 0.0916586993886271, 0.09141959863752146, 0.09118172163651435, 0.09094505681316464, 0.0907095926777681, 0.09047531784300686, 0.09024222103978558, 0.0900102911297657, 0.08977951711506571, 0.08954988814554933, 0.0893213935240775, 0.08909402271005512, 0.08886776532156297, 0.08864261113632568, 0.08841855009173225, 0.08819557228409386, 0.0879736679672966, 0.08775282755098097, 0.08753304159836009, 0.08731430082376827, 0.08709659609001762, 0.08687991840562427, 0.08666425892195662, 0.08644960893034544, 0.08623595985918978, 0.08602330327108346, 0.08581163085998254, 0.08560093444842799, 0.08539120598483492, 0.08518243754085537, 0.08497462130881932, 0.08476774959925637, 0.08456181483849909, 0.08435680956636643, 0.08415272643392624, 0.08394955820133361, 0.08374729773574212, 0.08354593800928409, 0.08334547209711624, 0.08314589317552647, 0.08294719452009772, 0.08274936950392577, 0.0825524115958861, 0.08235631435894757, 0.08216107144852888, 0.08196667661089557, 0.08177312368159495, 0.08158040658392658, 0.08138851932744738, 0.08119745600650911, 0.08100721079882833, 0.08081777796408712, 0.08062915184256549, 0.0804413268538045, 0.08025429749530116, 0.08006805834123508, 0.07988260404122748, 0.07969792931913412, 0.07951402897187157, 0.07933089786827852, 0.07914853094801255, 0.07896692322048235, 0.0787860697638162, 0.07860596572386594, 0.07842660631324626, 0.07824798681040855, 0.07807010255874706, 0.0778929489657363, 0.07771652150209639, 0.07754081570098337, 0.07736582715720082, 0.07719155152642794, 0.07701798452445914, 0.07684512192644968, 0.07667295956616066, 0.07650149333519686, 0.07633071918223047, 0.07616063311220268, 0.07599123118549607, 0.07582250951706945, 0.075654464275548, 0.07548709168226071, 0.07532038801021823, 0.07515434958302458, 0.07498897277371662, 0.07482425400352646, 0.07466018974056257, 0.07449677649840693, 0.07433401083462661, 0.07417188934919974, 0.07401040868285699, 0.07384956551534172, 0.07368935656359342, 0.0735297785798602, 0.07337082834974812, 0.07321250269021662, 0.07305479844752998, 0.07289771249517703, 0.07274124173177117, 0.07258538307894451, 0.07243013347924977, 0.0722754898940846, 0.07212144930165247, 0.07196800869497438, 0.07181516507996533, 0.07166291547358837, 0.07151125690209892, 0.07136018639938987, 0.07120970100544777, 0.07105979776492856, 0.07091047372585943, 0.07076172593847256, 0.07061355145417429, 0.0704659473246515, 0.07031891060111675, 0.07017243833368973, 0.07002652757091402, 0.06988117535940418, 0.06973637874361945, 0.06959213476575678, 0.06944844046575764, 0.06930529288142002, 0.06916268904860795, 0.06902062600155005, 0.06887910077321796, 0.0687381103957761, 0.06859765190109358, 0.06845772232130962, 0.06831831868944391, 0.06817943804004355, 0.06804107740985908, 0.06790323383854191, 0.0677659043693562, 0.06762908604989928, 0.06749277593282443, 0.06735697107656091, 0.06722166854602643, 0.06708686541332835, 0.06695255875844944, 0.06681874566991548, 0.0666854232454422, 0.06655258859255901, 0.0664202388292083, 0.06628837108431862, 0.06615698249835104, 0.06602607022381794, 0.065895631425774, 0.06576566328227922, 0.06563616298483417, 0.06550712773878788, 0.06537855476371889, 0.06525044129379005, 0.06512278457807799, 0.06499558188087817, 0.06486883048198623, 0.06474252767695707, 0.0646166707773424, 0.0644912571109082, 0.06436628402183268, 0.06424174887088659, 0.06411764903559633, 0.06399398191039148, 0.06387074490673732, 0.0637479354532541, 0.06362555099582325, 0.06350358899768228, 0.06338204693950877, 0.06326092231949479, 0.063140212653412, 0.06301991547466919, 0.06290002833436208, 0.06278054880131662, 0.06266147446212636, 0.06254280292118461, 0.06242453180071159, 0.06230665874077787, 0.062189181399323626, 0.062072097452174964, 0.06195540459305734, 0.061839100533606556, 0.0617231830033775, 0.06160764974985129, 0.061492498538440576, 0.061377727152493664, 0.061263333393297446, 0.06114931508007931, 0.061035670050008095, 0.06092239615819436, 0.06080949127768995, 0.06069695329948675, 0.060584780132515045, 0.06047296970364096, 0.06036151995766339, 0.060250428857310155, 0.0601396943832335, 0.06002931453400456, 0.05991928732610693, 0.059809610793929255, 0.05970028298975659, 0.059591301983760364, 0.05948266586398696, 0.05937437273634462, 0.059266420724588605, 0.05915880797030427, 0.05905153263288814, 0.058944592889526526, 0.05883798693517164, 0.058731712982515054, 0.05862576926195809, 0.058520154021579424, 0.05841486552709886, 0.05830990206183822, 0.058205261926678076, 0.05810094344001082, 0.05799694493768981, 0.05789326477297392, 0.057789901316468266, 0.05768685295605986, 0.05758411809684896, 0.0574816951610755, 0.05737958258804057, 0.05727777883402295, 0.057176282372190385, 0.057075091692505873, 0.05697420530162854, 0.056873621722809235, 0.056773339495780846, 0.05667335717664321, 0.05657367333774285, 0.056474286567547195, 0.05637519547051363, 0.05627639866695348, 0.056177894792890634, 0.05607968249991541, 0.05598176045503338, 0.055884127340509494, 0.05578678185370759, 0.05568972270692553, 0.055592948627226066, 0.0554964583562637, 0.05540025065010773, 0.05530432427906177, 0.05520867802747989, 0.05511331069357977, 0.0550182210892529, 0.0549234080398725, 0.05482887038409905, 0.054734606973683875, 0.0546406166732711, 0.05454689836019838, 0.05445345092429631, 0.05436027326768728, 0.05426736430458387, 0.05417472296108692, 0.054082348174983745, 0.053990238895546896, 0.05389839408333333, 0.05380681270998459, 0.05371549375802814, 0.053624436220680124, 0.05353363910164951, 0.05344310141494421, 0.05335282218467922, 0.05326280044488666, 0.05317303523932832, 0.05308352562131051, 0.05299427065350145, 0.05290526940775138, 0.05281652096491505, 0.052728024414677395, 0.05263977885538154, 0.05255178339385981, 0.05246403714526742, 0.05237653923291896, 0.05228928878812749, 0.052202284950046175, 0.05211552686551274, 0.052029013688896064, 0.0519427445819452, 0.05185671871364075, 0.051770935260048015, 0.051685393404172345, 0.05160009233581618, 0.05151503125143756, 0.05143020935401034, 0.051345625852885476, 0.051261279963653456, 0.051177170908007816, 0.051093297913609205, 0.05100966021395021, 0.05092625704822057, 0.050843087661172695, 0.05076015130298734, 0.05067744722913926, 0.050594974700262765, 0.05051273298201714, 0.05043072134495165, 0.05034893906437011, 0.05026738542019531, 0.05018605969683245, 0.050104961183032634, 0.05002408917175523, 0.049943442960030296, 0.04986302184882011, 0.04978282514288045, 0.04970285215062156, 0.04962310218396873, 0.04954357455822276, 0.04946426859192056, 0.04938518360669565, 0.04930631892713906, 0.0492276738806608, 0.049149247797351685, 0.04907104000984655, 0.04899304985318788, 0.0489152766646914, 0.04883771978381275, 0.0487603785520162, 0.04868325231264527, 0.048606340410795645, 0.04852964219319068, 0.04845315700805949, 0.048376884205018275, 0.04830082313495465, 0.048224973149915557, 0.048149333602998695, 0.04807390384824811, 0.04799868324055351, 0.04792367113555389, 0.04784886688954576, 0.04777426985939563, 0.04769987940245727, 0.047625694876493756, 0.0475517156396043, 0.04747794105015605, 0.04740437046672074, 0.047331003248016534, 0.04725783875285464, 0.04718487634009101, 0.047112115368583056, 0.04703955519715112, 0.04696719518454485, 0.04689503468941437, 0.04682307307028598, 0.04675130968554248, 0.0466797438934078, 0.04660837505193598, 0.0465372025190041, 0.046466225652309276, 0.04639544380936938, 0.04632485634752716, 0.04625446262395802, 0.04618426199568072, 0.04611425381957116, 0.046044437452379045, 0.04597481225074695, 0.04590537757123181, 0.04583613277032869, 0.04576707720449649, 0.04569821023018529, 0.0456295312038655, 0.04556103948205818, 0.045492734421366914, 0.04542461537851032, 0.0453566817103558, 0.04528893277395377, 0.04522136792657263, 0.04515398652573397, 0.04508678792924832, 0.04501977149525073, 0.04495293658223681, 0.044886282549098246, 0.04481980875515851, 0.04475351456020815, 0.04468739932453978, 0.044621462408982564, 0.0445557031749364, 0.04449012098440534, 0.044424715200030614, 0.044359485185122866, 0.04429443030369375, 0.04422954992048689, 0.04416484340100799, 0.04410031011155429, 0.04403594941924319, 0.043971760692040014, 0.04390774329878534, 0.043843896609221014, 0.04378021999401578, 0.043716712824790074, 0.04365337447413988, 0.04359020431565997, 0.04352720172396642, 0.04346436607471834, 0.04340169674463879, 0.04333919311153532, 0.04327685455431945, 0.043214680453025796, 0.04315267018883041, 0.04309082314406858, 0.043029138702251904, 0.04296761624808508, 0.04290625516748183, 0.04284505484758044, 0.042784014676758825, 0.042723134044648975, 0.042662412342151124, 0.042601848961447225, 0.04254144329601411, 0.04248119474063625, 0.042421102691418036, 0.042361166545795644, 0.04230138570254861, 0.042241759561810914, 0.04218228752508165, 0.04212296899523558, 0.04206380337653305, 0.04200479007462969, 0.04194592849658581, 0.041887218050875315, 0.04182865814739442, 0.041770248197469904, 0.04171198761386718, 0.04165387581079789, 0.04159591220392726, 0.041538096210381015, 0.04148042724875207, 0.04142290473910687, 0.041365528102991225, 0.04130829676343607, 0.04125121014496263, 0.04119426767358735, 0.041137468776826495, 0.041080812883700274]\n",
            "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CIBXyZO68hRr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "32e4d7fa-4409-46cf-e095-8f1231506b85"
      },
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "for i in range(len(p)):\n",
        "  if p.tolist()[i] == y[1700:y.shape[0]].tolist()[i]:\n",
        "    correct += 1\n",
        "\n",
        "print(\"The model is \" + str((correct/len(p))*100) + \"% accurate\")  "
      ],
      "execution_count": 652,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model is 93.81443298969072% accurate\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}